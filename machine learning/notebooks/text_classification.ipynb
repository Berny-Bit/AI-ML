{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a38836-8da1-40ba-8b67-b4b998e2220a",
   "metadata": {},
   "source": [
    "goal: based on the lyrics, predict the genre of the song. this might be really stupid, dont know, lets figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a8c06-5926-4efc-a0f7-f598083ba00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb65bfb-11e2-4cf5-bc77-54b343602465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('songs.csv', sep=',', header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67af0f9-3e62-4b60-948c-d4b4b6dedfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb7e76-1258-4ebf-ba8d-e99ba6fcda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb370dd-5e1a-4a8e-892b-93cf18f87b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['playlist_genre'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c855a7-8733-41c4-963e-e29b3e76c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['playlist_genre','lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d9f64-5b2d-4679-8033-e86ddb31a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0fb0c-5069-4ab2-81b5-005dad4384fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['lyrics']\n",
    "y = df['playlist_genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec8277-5503-4103-8310-fc22759f6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels into numerical values\n",
    "encoded_labels = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a35a9-c850-4b54-b383-01c173ad2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d0740-d47e-481d-bb89-ca09477ef143",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d120e-0d4a-49f1-87ee-f5bd9269fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0d643-639b-45be-b520-bb029ee637f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer to convert text to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805435b6-5873-409b-98d9-5ef9ad1e2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = pd.unique(df['playlist_genre']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b47c46-e9be-44e4-9a03-4af9d9d6c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78615dd0-c354-43ac-b936-7402d2d33dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=unique_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f38c3-3245-4663-a1ec-b9386019b131",
   "metadata": {},
   "source": [
    "These results represent the performance metrics of a music genre classification task. Let's break down the meaning of each concept:\n",
    "\n",
    "1. Accuracy: 0.59\n",
    "   - This indicates that the overall accuracy of the classification model is 59%. In other words, the model correctly classified 59% of all the music samples across all genres.\n",
    "\n",
    "2. Classification Report:\n",
    "   This report provides detailed metrics for each music genre class and overall averages.\n",
    "\n",
    "   a) Precision:\n",
    "      - Measures the accuracy of positive predictions.\n",
    "      - For example, Rock has a precision of 0.60, meaning 60% of the tracks the model labeled as Rock were actually Rock.\n",
    "\n",
    "   b) Recall:\n",
    "      - Measures the proportion of actual positives that were correctly identified.\n",
    "      - For Rock, the recall is 0.86, indicating that 86% of all actual Rock tracks were correctly identified by the model.\n",
    "\n",
    "   c) F1-score:\n",
    "      - The harmonic mean of precision and recall, providing a single score that balances both metrics.\n",
    "      - Rock has the highest F1-score of 0.71, suggesting it's the best-performing category.\n",
    "\n",
    "   d) Support:\n",
    "      - The number of samples for each class in the test set.\n",
    "      - Rock has the highest support with 86 samples, while Pop has only 5.\n",
    "\n",
    "3. Macro avg:\n",
    "   - The unweighted mean of the metrics for all classes.\n",
    "   - Precision: 0.60, Recall: 0.43, F1-score: 0.43\n",
    "\n",
    "4. Weighted avg:\n",
    "   - The weighted average of the metrics, taking class imbalance into account.\n",
    "   - Precision: 0.63, Recall: 0.59, F1-score: 0.55\n",
    "\n",
    "5. Performance by genre:\n",
    "   - Rock and Rap perform relatively well.\n",
    "   - R&B has high precision but low recall.\n",
    "   - EDM has perfect precision but very low recall.\n",
    "   - Latin music was not correctly classified at all (0.00 for all metrics).\n",
    "\n",
    "These results suggest that the model performs moderately well overall, but struggles with certain genres, particularly Latin music. There's room for improvement, especially in balancing precision and recall across all classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1cad7-9dab-4b7a-bc40-34b5066fd4f9",
   "metadata": {},
   "source": [
    "Here are actionable steps to improve your classification results based on the report:\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Key Issues Identified**\n",
    "1. **Severe class imbalance**: Rock (86 samples) vs. Latin (12) and EDM (13)\n",
    "2. **Complete failure in Latin** (0% precision/recall)\n",
    "3. **Low recall in EDM** (8%) and R&B (33%)\n",
    "4. **Moderate performance** in Pop/Rap\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è **Improvement Strategies**\n",
    "\n",
    "#### üìä **Data-Level Fixes**\n",
    "1. **Resampling Techniques**\n",
    "   - Apply **SMOTE** for Latin/EDM classes\n",
    "   - Use **class weights** (e.g., `class_weight=\"balanced\"` in sklearn)\n",
    "   - Strategic oversampling: Duplicate key Latin tracks\n",
    "\n",
    "2. **Data Augmentation**\n",
    "   - For audio data: Pitch shifts, tempo changes, noise injection\n",
    "   - For tabular data: Synthetic minority oversampling\n",
    "\n",
    "#### üß† **Model-Level Adjustments**\n",
    "1. **Algorithm Selection**\n",
    "   - Try **XGBoost/LightGBM** with `scale_pos_weight` parameter\n",
    "   - Experiment with **neural networks** (particularly effective for imbalanced data)\n",
    "   - Test **One-vs-Rest** approach for problematic classes\n",
    "\n",
    "2. **Threshold Tuning**\n",
    "   - Adjust decision thresholds per-class using ROC curves\n",
    "   ```python\n",
    "   from sklearn.calibration import calibration_curve\n",
    "   # Particularly impactful for EDM (high precision, low recall)\n",
    "   ```\n",
    "\n",
    "#### ‚öôÔ∏è **Feature Engineering**\n",
    "1. **Genre-Specific Features**\n",
    "   - For Latin: Add rhythmic complexity metrics\n",
    "   - For EDM: Include tempo stability features\n",
    "   - For R&B: Vocal harmony analysis\n",
    "\n",
    "2. **Dimensionality Reduction**\n",
    "   - Use **t-SNE** visualization to check class separability\n",
    "   - Apply **PCA** to eliminate redundant features\n",
    "\n",
    "#### üìà **Evaluation Enhancements**\n",
    "1. **Custom Metric Focus**\n",
    "   ```python\n",
    "   scoring = {\n",
    "       'f1_Latin': make_scorer(f1_score, labels=['Latin'], average='micro'),\n",
    "       'recall_EDM': make_scorer(recall_score, labels=['EDM'], average='micro')\n",
    "   }\n",
    "   ```\n",
    "2. **Error Analysis**\n",
    "   - Create confusion matrix with absolute counts:\n",
    "   ```\n",
    "   Rock ‚Üí Latin: 9 misclassifications\n",
    "   EDM ‚Üí Rock: 11 misclassifications \n",
    "   Latin ‚Üí Pop: 8 misclassifications\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Immediate Action Items**\n",
    "1. **Priority 1**: Fix Latin classification\n",
    "   - Collect 30-50 additional Latin samples\n",
    "   - Create separate binary classifier for Latin vs Rest\n",
    "\n",
    "2. **Priority 2**: Boost EDM/R&B recall\n",
    "   - Implement **cost-sensitive learning** with 5√ó penalty for EDM/R&B errors\n",
    "   - Add EDM-specific temporal features (drop intensity, beat consistency)\n",
    "\n",
    "3. **Quick Win**: Ensemble Methods\n",
    "   ```python\n",
    "   from sklearn.ensemble import StackingClassifier\n",
    "   # Combine SVM (good for Rock) with KNN (better for small classes)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Expected Outcome**\n",
    "After implementation, target these improvements:\n",
    "| Class  | Current F1 | Target F1 |\n",
    "|--------|------------|-----------|\n",
    "| Latin  | 0.00       | 0.45+     |\n",
    "| EDM    | 0.14       | 0.35+     |\n",
    "| R&B    | 0.47       | 0.60+     |\n",
    "| **Overall Accuracy** | 59% | 68%+ |\n",
    "\n",
    "Pro Tip: Use **Bayesian optimization** for hyperparameter tuning rather than grid search for faster convergence. Focus on optimizing for weighted F1 rather than accuracy.\n",
    "\n",
    "---\n",
    "Antwoord van Perplexity: pplx.ai/share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b8351-87e7-41c2-93b3-da30c4318e01",
   "metadata": {},
   "source": [
    "### Resampling \n",
    "restart kernel and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46314fd-f501-4379-b81c-28705f53d42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.60      0.60      0.60         5\n",
      "       Latin       0.60      0.68      0.64        22\n",
      "         Pop       0.60      0.86      0.71        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       0.78      0.33      0.47        42\n",
      "        Rock       1.00      0.08      0.14        13\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.60      0.43      0.43       180\n",
      "weighted avg       0.63      0.59      0.55       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('songs.csv', sep=',', header='infer')\n",
    "\n",
    "# 1. Prepare the data\n",
    "X = df['lyrics']\n",
    "y = df['playlist_genre']\n",
    "\n",
    "# 2. Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 3. Split data with index reset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train = X_train.reset_index(drop=True)  # Fix indexing\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "# 4. Vectorize text\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()  # Convert to dense array\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Apply SMOTE only to Latin and EDM classes\n",
    "# Get encoded values for target classes\n",
    "latin_label = label_encoder.transform(['Latin'])[0]\n",
    "edm_label = label_encoder.transform(['Edm'])[0]\n",
    "\n",
    "sm = SMOTE(sampling_strategy={\n",
    "    latin_label: 103,  # Create 100 samples for Latin\n",
    "    edm_label: 103     # Create 100 samples for EDM\n",
    "}, random_state=42)\n",
    "\n",
    "X_res, y_res = sm.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# 6. Train and evaluate\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_res, y_res)\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# 7. Show results\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83fc44-d860-4d55-8511-cfa078bedeab",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "    Accuracy Remains Unchanged: The overall accuracy is still around 0.59, suggesting that SMOTE might not have effectively improved the model's performance.\n",
    "\n",
    "    Class Imbalance: Despite SMOTE, the classification report shows that some classes (like R&B) still have poor metrics, indicating that the imbalance issue persists.\n",
    "\n",
    "    SMOTE Strategy: The current SMOTE strategy targets Latin and EDM classes but might not be sufficient for other minority classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92262910-08af-40c4-8051-44f7cba8f956",
   "metadata": {},
   "source": [
    "Reset the kernel and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49387181-7d07-402f-a76d-bf45cb25a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.60      0.60      0.60         5\n",
      "       Latin       0.60      0.68      0.64        22\n",
      "         Pop       0.59      0.80      0.68        86\n",
      "         R&b       0.08      0.08      0.08        12\n",
      "         Rap       0.76      0.31      0.44        42\n",
      "        Rock       0.25      0.08      0.12        13\n",
      "\n",
      "    accuracy                           0.57       180\n",
      "   macro avg       0.48      0.43      0.43       180\n",
      "weighted avg       0.57      0.57      0.54       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('songs.csv', sep=',', header='infer')\n",
    "\n",
    "# Prepare data\n",
    "X = df['lyrics']\n",
    "y = df['playlist_genre']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Identify minority classes\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "minority_classes = class_counts[class_counts < class_counts.max() * 0.2].index\n",
    "\n",
    "# Apply SMOTE to minority classes\n",
    "sm = SMOTE(sampling_strategy={c: 100 for c in minority_classes}, random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Train and evaluate\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_res, y_res)\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# Show results\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270140d4-fe2c-454e-957c-d70d52465252",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "    Decreased Accuracy: The overall accuracy has decreased slightly, suggesting that the SMOTE application might have introduced noise or overfitting.\n",
    "\n",
    "    Class Imbalance Persists: Despite targeting minority classes, the metrics for classes like R&B and Rock remain poor.\n",
    "\n",
    "    Precision and Recall Issues: Low precision and recall for several classes indicate that the model is struggling to accurately classify these classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6885db22-cdd5-447e-beb4-abce8897d1f0",
   "metadata": {},
   "source": [
    "Reset the kernel and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d634a52f-9e54-4448-8c9c-5d44a0fe9327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.75      0.60      0.67         5\n",
      "       Latin       0.60      0.68      0.64        22\n",
      "         Pop       0.62      0.93      0.75        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       1.00      0.52      0.69        42\n",
      "        Rock       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.67       180\n",
      "   macro avg       0.50      0.46      0.46       180\n",
      "weighted avg       0.63      0.67      0.61       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('songs.csv', sep=',', header='infer')\n",
    "\n",
    "# Prepare data\n",
    "X = df['lyrics']\n",
    "y = df['playlist_genre']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Apply SMOTE to minority classes\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "minority_classes = class_counts[class_counts < class_counts.max() * 0.2].index\n",
    "sm = SMOTE(sampling_strategy={c: 100 for c in minority_classes}, random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Define hyperparameter tuning space for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_res, y_res)\n",
    "\n",
    "# Train and evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "# Show results\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea492554-50f8-4436-828d-ac9cfa4fafbe",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "    Improved Accuracy: The overall accuracy has increased to 0.67, which is a positive step.\n",
    "\n",
    "    Class-Specific Issues: Despite improvements, classes like R&B and Rock still have poor metrics (precision and recall of 0.00).\n",
    "\n",
    "    Precision and Recall Variability: There's variability in precision and recall across classes, indicating that the model is performing well on some classes but not others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d42fd-7651-4fcd-9a90-85b99896de78",
   "metadata": {},
   "source": [
    "reset kernel and run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007d57f1-bd73-4601-81d8-1c80e2c76cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (SMOTE): 0.67\n",
      "\n",
      "Classification Report (SMOTE):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.75      0.60      0.67         5\n",
      "       Latin       0.60      0.68      0.64        22\n",
      "         Pop       0.62      0.93      0.75        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       1.00      0.52      0.69        42\n",
      "        Rock       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.67       180\n",
      "   macro avg       0.50      0.46      0.46       180\n",
      "weighted avg       0.63      0.67      0.61       180\n",
      "\n",
      "Accuracy (EasyEnsembleClassifier): 0.48\n",
      "\n",
      "Classification Report (EasyEnsembleClassifier):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       1.00      0.20      0.33         5\n",
      "       Latin       0.50      0.68      0.58        22\n",
      "         Pop       0.63      0.51      0.56        86\n",
      "         R&b       0.11      0.42      0.18        12\n",
      "         Rap       0.83      0.45      0.58        42\n",
      "        Rock       0.27      0.23      0.25        13\n",
      "\n",
      "    accuracy                           0.48       180\n",
      "   macro avg       0.56      0.42      0.41       180\n",
      "weighted avg       0.61      0.48      0.52       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('songs.csv', sep=',', header='infer')\n",
    "\n",
    "# Prepare data\n",
    "X = df['lyrics']\n",
    "y = df['playlist_genre']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Apply SMOTE to minority classes\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "minority_classes = class_counts[class_counts < class_counts.max() * 0.2].index\n",
    "sm = SMOTE(sampling_strategy={c: 100 for c in minority_classes}, random_state=42)\n",
    "X_res_smote, y_res_smote = sm.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Apply ADASYN for comparison\n",
    "ad = ADASYN(sampling_strategy={c: 100 for c in minority_classes}, random_state=42)\n",
    "X_res_adasyn, y_res_adasyn = ad.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Define hyperparameter tuning space for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_res_smote, y_res_smote)\n",
    "\n",
    "# Train and evaluate the best model\n",
    "best_model_smote = grid_search.best_estimator_\n",
    "y_pred_smote = best_model_smote.predict(X_test_vec)\n",
    "\n",
    "# Use EasyEnsembleClassifier for robust handling of class imbalance\n",
    "eec = EasyEnsembleClassifier(random_state=42)\n",
    "eec.fit(X_res_smote, y_res_smote)\n",
    "y_pred_ee = eec.predict(X_test_vec)\n",
    "\n",
    "# Show results\n",
    "print(f\"Accuracy (SMOTE): {accuracy_score(y_test, y_pred_smote):.2f}\")\n",
    "print(\"\\nClassification Report (SMOTE):\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred_smote, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n",
    "\n",
    "print(f\"Accuracy (EasyEnsembleClassifier): {accuracy_score(y_test, y_pred_ee):.2f}\")\n",
    "print(\"\\nClassification Report (EasyEnsembleClassifier):\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred_ee, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c37432-6f82-4671-92fd-027676d82a6f",
   "metadata": {},
   "source": [
    "SMOTE Results:\n",
    "\n",
    "    Accuracy: The model achieves an accuracy of 0.67, which is relatively good considering the class imbalance.\n",
    "\n",
    "    Class Performance:\n",
    "\n",
    "        Edm: High precision (0.75) but moderate recall (0.60), indicating some missed classifications.\n",
    "\n",
    "        Latin: Balanced metrics (precision: 0.60, recall: 0.68) suggest reasonable performance.\n",
    "\n",
    "        Pop: High recall (0.93) but moderate precision (0.62), indicating some false positives.\n",
    "\n",
    "        R&b and Rock: Both have poor metrics (precision and recall of 0.00), indicating significant misclassification.\n",
    "\n",
    "        Rap: High precision (1.00) but low recall (0.52), suggesting missed classifications.\n",
    "\n",
    "    Macro and Weighted Averages: The macro average metrics are lower than the weighted averages, indicating that the model performs better on larger classes.\n",
    "\n",
    "EasyEnsembleClassifier Results:\n",
    "\n",
    "    Accuracy: The accuracy is significantly lower at 0.48, suggesting that this approach might not be as effective for this dataset.\n",
    "\n",
    "    Class Performance:\n",
    "\n",
    "        Edm: High precision (1.00) but very low recall (0.20), indicating many missed classifications.\n",
    "\n",
    "        Latin: Moderate metrics (precision: 0.50, recall: 0.68) are somewhat balanced.\n",
    "\n",
    "        Pop: Lower recall (0.51) compared to SMOTE, indicating more missed classifications.\n",
    "\n",
    "        R&b: Slightly improved metrics compared to SMOTE, but still poor.\n",
    "\n",
    "        Rap: Lower precision (0.83) and recall (0.45) compared to SMOTE.\n",
    "\n",
    "        Rock: Slightly improved metrics but still poor.\n",
    "\n",
    "    Macro and Weighted Averages: The macro average metrics are higher than in the SMOTE case but still indicate overall poor performance.\n",
    "\n",
    "Overall, the SMOTE approach seems to perform better for this dataset, especially in terms of overall accuracy and performance on larger classes. However, both methods struggle with minority classes like R&B and Rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a6ef75-6994-4925-b109-0cc56d8eef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.66\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       1.00      0.60      0.75         5\n",
      "       Latin       0.58      0.68      0.62        22\n",
      "         Pop       0.61      0.92      0.73        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       1.00      0.50      0.67        42\n",
      "        Rock       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.66       180\n",
      "   macro avg       0.53      0.45      0.46       180\n",
      "weighted avg       0.62      0.66      0.60       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.63\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       1.00      0.60      0.75         5\n",
      "       Latin       0.59      0.59      0.59        22\n",
      "         Pop       0.62      0.88      0.73        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       0.81      0.50      0.62        42\n",
      "        Rock       0.25      0.08      0.12        13\n",
      "\n",
      "    accuracy                           0.63       180\n",
      "   macro avg       0.55      0.44      0.47       180\n",
      "weighted avg       0.60      0.63      0.59       180\n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.64\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.00      0.00      0.00         5\n",
      "       Latin       0.60      0.68      0.64        22\n",
      "         Pop       0.60      0.93      0.73        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       0.95      0.50      0.66        42\n",
      "        Rock       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.64       180\n",
      "   macro avg       0.36      0.35      0.34       180\n",
      "weighted avg       0.58      0.64      0.58       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.63\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.00      0.00      0.00         5\n",
      "       Latin       0.61      0.64      0.62        22\n",
      "         Pop       0.59      0.92      0.72        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       0.88      0.50      0.64        42\n",
      "        Rock       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.63       180\n",
      "   macro avg       0.35      0.34      0.33       180\n",
      "weighted avg       0.56      0.63      0.57       180\n",
      "\n",
      "\n",
      "Model: Multinomial Naive Bayes\n",
      "Accuracy: 0.55\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.00      0.00      0.00         5\n",
      "       Latin       0.61      0.64      0.62        22\n",
      "         Pop       0.53      0.95      0.68        86\n",
      "         R&b       0.00      0.00      0.00        12\n",
      "         Rap       1.00      0.07      0.13        42\n",
      "        Rock       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.55       180\n",
      "   macro avg       0.36      0.28      0.24       180\n",
      "weighted avg       0.56      0.55      0.43       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.59\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Edm       0.60      0.60      0.60         5\n",
      "       Latin       0.50      0.36      0.42        22\n",
      "         Pop       0.63      0.83      0.71        86\n",
      "         R&b       0.14      0.08      0.11        12\n",
      "         Rap       0.59      0.55      0.57        42\n",
      "        Rock       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.41      0.40      0.40       180\n",
      "weighted avg       0.53      0.59      0.55       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\bernw\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('songs.csv', sep=',', header='infer')\n",
    "\n",
    "# Prepare data\n",
    "X = df['lyrics']\n",
    "y = df['playlist_genre']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Define and train models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    \n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_test, \n",
    "        y_pred, \n",
    "        target_names=label_encoder.classes_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3511cf-59e6-4b60-8782-6e03f0081969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
